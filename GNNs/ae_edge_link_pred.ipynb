{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bb92138",
   "metadata": {},
   "source": [
    "[link prediction example](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/link_pred.py)\n",
    "\n",
    "- kernel on macos is `torosx`\n",
    "- Next, work with [3] to better make use of the random link split  \n",
    "\n",
    "\n",
    "[1]: https://antoniolonga.github.io/Pytorch_geometric_tutorials/posts/post6.html\n",
    "[2]: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.transforms.RandomLinkSplit.html#torch_geometric.transforms.RandomLinkSplit\n",
    "[3]: https://github.com/pyg-team/pytorch_geometric/blob/master/examples/autoencoder.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fb2622-f462-4f1d-8d66-3cca5f940cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: \\ "
     ]
    }
   ],
   "source": [
    "# Troubleshooting \n",
    "# ImportError: 'scatter' requires the 'torch-scatter' package\n",
    "!conda install pytorch-scatter -c pyg -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4d772a1-f8b2-43bc-9f91-2ba5e9ac52d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saguinaga/opt/anaconda3/envs/torosx/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric import transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from torch_geometric.nn import GAE, VGAE, GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab112273-469e-4019-ad05-767a6683274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f5dd6f-54df-4817-9b13-169f910b6c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "def download_citeseer_dataset():\n",
    "    dataset = Planetoid(\"./Data\",\"CiteSeer\", transform=T.NormalizeFeatures())\n",
    "    return dataset\n",
    "\n",
    "# dataset = download_citeseer_dataset()\n",
    "# data = dataset[0]\n",
    "# data.train_mask = data.val_mask =data.test_mask = None\n",
    "# print(data)\n",
    "# data = train_test_split_edges(data)\n",
    "# print(data)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2ad81c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 8976], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[4488], edge_label_index=[2, 4488])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "# (Data(x=[3327, 3703], edge_index=[2, 6374], y=[3327], train_mask=[3327],val_mask=[3327], test_mask=[3327], edge_label=[6374], edge_label_index=[2, 6374]), \n",
    "# Data(x=[3327, 3703], edge_index=[2, 6374], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327], edge_label=[910], edge_label_index=[2, 910]), \n",
    "# Data(x=[3327, 3703], edge_index=[2, 7284], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327], edge_label=[1820], edge_label_index=[2, 1820]))\n",
    "\n",
    "# transform = RandomLinkSplit(is_undirected=True)\n",
    "\n",
    "# # data.train_mask = data.val_mask =data.test_mask = None\n",
    "# # data = transform(data)\n",
    "\n",
    "# train_data, val_data, test_data = transform(data)\n",
    "# train_data.train_mask = val_data.val_mask =test_data.test_mask = None\n",
    "transform = T.Compose([\n",
    "    T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False),\n",
    "])\n",
    "# path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'Planetoid')\n",
    "dataset = Planetoid('./Data', name='Cora', transform=transform)\n",
    "# After applying the `RandomLinkSplit` transform, the data is transformed from\n",
    "# a data object to a list of tuples (train_data, val_data, test_data), with\n",
    "# each element representing the corresponding split.\n",
    "train_data, val_data, test_data = dataset[0]\n",
    "\n",
    "\n",
    "\n",
    "# transform = T.Compose([\n",
    "#     T.NormalizeFeatures(),\n",
    "#     T.ToDevice(device),\n",
    "#     T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "#                       split_labels=True, add_negative_train_samples=False),\n",
    "# ])\n",
    "# # path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'Planetoid')\n",
    "# # dataset = Planetoid(path, args.dataset, transform=transform)\n",
    "\n",
    "# dataset = Planetoid(\"./Data\",\"CiteSeer\", transform=transform)\n",
    "# train_data, val_data, test_data = dataset[0]\n",
    "\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3dd6e6-dac1-430e-8194-b95d8fafe68a",
   "metadata": {},
   "source": [
    "- train data has all edge labels set to Positve or 1.0\n",
    "- test data has a balanced set of edge labels (527)\n",
    "- train data has 4488 edge labels out of 8976 total edges (from `edge_index`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "654f9019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3    818\n",
       " 4    426\n",
       " 2    418\n",
       " 0    351\n",
       " 5    298\n",
       " 1    217\n",
       " 6    180\n",
       " Name: count, dtype: int64,\n",
       " 1.0    4488\n",
       " Name: count, dtype: int64,\n",
       " 1.0    527\n",
       " 0.0    527\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_data.y).value_counts(), pd.Series(train_data.edge_label).value_counts(), pd.Series(test_data.edge_label).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "003e3df3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "'scatter' requires the 'torch-scatter' package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m best_val_auc \u001b[38;5;241m=\u001b[39m final_test_auc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m101\u001b[39m):\n\u001b[0;32m---> 60\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     val_auc \u001b[38;5;241m=\u001b[39m test(val_data)\n\u001b[1;32m     62\u001b[0m     test_auc \u001b[38;5;241m=\u001b[39m test(test_data)\n",
      "Cell \u001b[0;32mIn[14], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 27\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# We perform a new round of negative sampling for every training epoch:\u001b[39;00m\n\u001b[1;32m     30\u001b[0m neg_edge_index \u001b[38;5;241m=\u001b[39m negative_sampling(\n\u001b[1;32m     31\u001b[0m     edge_index\u001b[38;5;241m=\u001b[39mtrain_data\u001b[38;5;241m.\u001b[39medge_index, num_nodes\u001b[38;5;241m=\u001b[39mtrain_data\u001b[38;5;241m.\u001b[39mnum_nodes,\n\u001b[1;32m     32\u001b[0m     num_neg_samples\u001b[38;5;241m=\u001b[39mtrain_data\u001b[38;5;241m.\u001b[39medge_label_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m, in \u001b[0;36mNet.encode\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index):\n\u001b[0;32m----> 8\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrelu()\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x, edge_index)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torosx/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torosx/lib/python3.8/site-packages/torch_geometric/nn/conv/gcn_conv.py:241\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    239\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mgcn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# yapf: disable\u001b[39;49;00m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimproved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_self_loops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached:\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torosx/lib/python3.8/site-packages/torch_geometric/nn/conv/gcn_conv.py:108\u001b[0m, in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m row, col \u001b[38;5;241m=\u001b[39m edge_index[\u001b[38;5;241m0\u001b[39m], edge_index[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    107\u001b[0m idx \u001b[38;5;241m=\u001b[39m col \u001b[38;5;28;01mif\u001b[39;00m flow \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_to_target\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m row\n\u001b[0;32m--> 108\u001b[0m deg \u001b[38;5;241m=\u001b[39m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m deg_inv_sqrt \u001b[38;5;241m=\u001b[39m deg\u001b[38;5;241m.\u001b[39mpow_(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m    110\u001b[0m deg_inv_sqrt\u001b[38;5;241m.\u001b[39mmasked_fill_(deg_inv_sqrt \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torosx/lib/python3.8/site-packages/torch_geometric/utils/_scatter.py:168\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m src\u001b[38;5;241m.\u001b[39mnew_zeros(size)\u001b[38;5;241m.\u001b[39mscatter_(dim, index, src)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch_geometric\u001b[38;5;241m.\u001b[39mtyping\u001b[38;5;241m.\u001b[39mWITH_TORCH_SCATTER:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscatter\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m requires the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch-scatter\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m package\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamin\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamax\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    171\u001b[0m     reduce \u001b[38;5;241m=\u001b[39m reduce[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m:]\n",
      "\u001b[0;31mImportError\u001b[0m: 'scatter' requires the 'torch-scatter' package"
     ]
    }
   ],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "\n",
    "model = Net(dataset.num_features, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "z = model.encode(test_data.x, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691531fd-ab84-4c7a-ba50-08c73ad47948",
   "metadata": {},
   "source": [
    "# End^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30e1abd-28ce-47b1-a501-4b0e530ddd94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af145d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels, out_channels = dataset.num_features, 6\n",
    "\n",
    "# if not args.variational and not args.linear:\n",
    "# model = GAE(GCNEncoder(in_channels, out_channels))\n",
    "# elif not args.variational and args.linear:\n",
    "# model = GAE(LinearEncoder(in_channels, out_channels))\n",
    "# elif args.variational and not args.linear:\n",
    "model = VGAE(VariationalGCNEncoder(in_channels, out_channels))\n",
    "# elif args.variational and args.linear:\n",
    "# model = VGAE(VariationalLinearEncoder(in_channels, out_channels))\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d56086d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, AUC: 0.6423, AP: 0.6687\n",
      "Epoch: 002, AUC: 0.6488, AP: 0.6721\n",
      "Epoch: 003, AUC: 0.6506, AP: 0.6724\n",
      "Epoch: 004, AUC: 0.6524, AP: 0.6736\n",
      "Epoch: 005, AUC: 0.6525, AP: 0.6740\n",
      "Epoch: 006, AUC: 0.6524, AP: 0.6740\n",
      "Epoch: 007, AUC: 0.6522, AP: 0.6740\n",
      "Epoch: 008, AUC: 0.6518, AP: 0.6738\n",
      "Epoch: 009, AUC: 0.6515, AP: 0.6736\n",
      "Epoch: 010, AUC: 0.6510, AP: 0.6732\n",
      "Epoch: 011, AUC: 0.6505, AP: 0.6725\n",
      "Epoch: 012, AUC: 0.6495, AP: 0.6714\n",
      "Epoch: 013, AUC: 0.6480, AP: 0.6703\n",
      "Epoch: 014, AUC: 0.6456, AP: 0.6687\n",
      "Epoch: 015, AUC: 0.6419, AP: 0.6664\n",
      "Epoch: 016, AUC: 0.6394, AP: 0.6657\n",
      "Epoch: 017, AUC: 0.6376, AP: 0.6649\n",
      "Epoch: 018, AUC: 0.6374, AP: 0.6651\n",
      "Epoch: 019, AUC: 0.6387, AP: 0.6656\n",
      "Epoch: 020, AUC: 0.6405, AP: 0.6667\n",
      "Epoch: 021, AUC: 0.6429, AP: 0.6679\n",
      "Epoch: 022, AUC: 0.6447, AP: 0.6692\n",
      "Epoch: 023, AUC: 0.6464, AP: 0.6703\n",
      "Epoch: 024, AUC: 0.6479, AP: 0.6714\n",
      "Epoch: 025, AUC: 0.6492, AP: 0.6723\n",
      "Epoch: 026, AUC: 0.6500, AP: 0.6730\n",
      "Epoch: 027, AUC: 0.6507, AP: 0.6736\n",
      "Epoch: 028, AUC: 0.6512, AP: 0.6744\n",
      "Epoch: 029, AUC: 0.6518, AP: 0.6750\n",
      "Epoch: 030, AUC: 0.6522, AP: 0.6756\n",
      "Epoch: 031, AUC: 0.6526, AP: 0.6760\n",
      "Epoch: 032, AUC: 0.6528, AP: 0.6762\n",
      "Epoch: 033, AUC: 0.6532, AP: 0.6764\n",
      "Epoch: 034, AUC: 0.6538, AP: 0.6770\n",
      "Epoch: 035, AUC: 0.6546, AP: 0.6776\n",
      "Epoch: 036, AUC: 0.6551, AP: 0.6782\n",
      "Epoch: 037, AUC: 0.6560, AP: 0.6789\n",
      "Epoch: 038, AUC: 0.6565, AP: 0.6794\n",
      "Epoch: 039, AUC: 0.6575, AP: 0.6805\n",
      "Epoch: 040, AUC: 0.6582, AP: 0.6813\n",
      "Epoch: 041, AUC: 0.6591, AP: 0.6823\n",
      "Epoch: 042, AUC: 0.6600, AP: 0.6832\n",
      "Epoch: 043, AUC: 0.6607, AP: 0.6842\n",
      "Epoch: 044, AUC: 0.6613, AP: 0.6849\n",
      "Epoch: 045, AUC: 0.6621, AP: 0.6861\n",
      "Epoch: 046, AUC: 0.6629, AP: 0.6871\n",
      "Epoch: 047, AUC: 0.6638, AP: 0.6881\n",
      "Epoch: 048, AUC: 0.6646, AP: 0.6893\n",
      "Epoch: 049, AUC: 0.6654, AP: 0.6902\n",
      "Epoch: 050, AUC: 0.6664, AP: 0.6916\n",
      "Epoch: 051, AUC: 0.6673, AP: 0.6928\n",
      "Epoch: 052, AUC: 0.6679, AP: 0.6939\n",
      "Epoch: 053, AUC: 0.6686, AP: 0.6953\n",
      "Epoch: 054, AUC: 0.6691, AP: 0.6964\n",
      "Epoch: 055, AUC: 0.6699, AP: 0.6979\n",
      "Epoch: 056, AUC: 0.6704, AP: 0.6992\n",
      "Epoch: 057, AUC: 0.6709, AP: 0.7004\n",
      "Epoch: 058, AUC: 0.6713, AP: 0.7017\n",
      "Epoch: 059, AUC: 0.6718, AP: 0.7037\n",
      "Epoch: 060, AUC: 0.6719, AP: 0.7050\n",
      "Epoch: 061, AUC: 0.6721, AP: 0.7066\n",
      "Epoch: 062, AUC: 0.6725, AP: 0.7081\n",
      "Epoch: 063, AUC: 0.6722, AP: 0.7091\n",
      "Epoch: 064, AUC: 0.6723, AP: 0.7102\n",
      "Epoch: 065, AUC: 0.6722, AP: 0.7114\n",
      "Epoch: 066, AUC: 0.6719, AP: 0.7126\n",
      "Epoch: 067, AUC: 0.6720, AP: 0.7139\n",
      "Epoch: 068, AUC: 0.6721, AP: 0.7156\n",
      "Epoch: 069, AUC: 0.6720, AP: 0.7169\n",
      "Epoch: 070, AUC: 0.6725, AP: 0.7181\n",
      "Epoch: 071, AUC: 0.6727, AP: 0.7194\n",
      "Epoch: 072, AUC: 0.6728, AP: 0.7202\n",
      "Epoch: 073, AUC: 0.6734, AP: 0.7215\n",
      "Epoch: 074, AUC: 0.6743, AP: 0.7228\n",
      "Epoch: 075, AUC: 0.6760, AP: 0.7245\n",
      "Epoch: 076, AUC: 0.6787, AP: 0.7265\n",
      "Epoch: 077, AUC: 0.6813, AP: 0.7281\n",
      "Epoch: 078, AUC: 0.6861, AP: 0.7311\n",
      "Epoch: 079, AUC: 0.6914, AP: 0.7342\n",
      "Epoch: 080, AUC: 0.6973, AP: 0.7373\n",
      "Epoch: 081, AUC: 0.7024, AP: 0.7403\n",
      "Epoch: 082, AUC: 0.7081, AP: 0.7435\n",
      "Epoch: 083, AUC: 0.7132, AP: 0.7466\n",
      "Epoch: 084, AUC: 0.7176, AP: 0.7496\n",
      "Epoch: 085, AUC: 0.7260, AP: 0.7548\n",
      "Epoch: 086, AUC: 0.7340, AP: 0.7601\n",
      "Epoch: 087, AUC: 0.7391, AP: 0.7640\n",
      "Epoch: 088, AUC: 0.7423, AP: 0.7667\n",
      "Epoch: 089, AUC: 0.7449, AP: 0.7690\n",
      "Epoch: 090, AUC: 0.7485, AP: 0.7714\n",
      "Epoch: 091, AUC: 0.7542, AP: 0.7747\n",
      "Epoch: 092, AUC: 0.7575, AP: 0.7766\n",
      "Epoch: 093, AUC: 0.7601, AP: 0.7782\n",
      "Epoch: 094, AUC: 0.7621, AP: 0.7795\n",
      "Epoch: 095, AUC: 0.7643, AP: 0.7808\n",
      "Epoch: 096, AUC: 0.7659, AP: 0.7819\n",
      "Epoch: 097, AUC: 0.7685, AP: 0.7834\n",
      "Epoch: 098, AUC: 0.7708, AP: 0.7846\n",
      "Epoch: 099, AUC: 0.7724, AP: 0.7853\n",
      "Epoch: 100, AUC: 0.7743, AP: 0.7863\n",
      "Median time per epoch: 0.0545s\n"
     ]
    }
   ],
   "source": [
    "args_variational = True\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "    loss = model.recon_loss(z, train_data.pos_edge_label_index)\n",
    "    if args_variational:\n",
    "        loss = loss + (1 / train_data.num_nodes) * model.kl_loss()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    return model.test(z, data.pos_edge_label_index, data.neg_edge_label_index)\n",
    "\n",
    "\n",
    "times = []\n",
    "for epoch in range(1, 100 + 1):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    auc, ap = test(test_data)\n",
    "    print(f'Epoch: {epoch:03d}, AUC: {auc:.4f}, AP: {ap:.4f}')\n",
    "    times.append(time.time() - start)\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59da57a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = dataset[0]\n",
    "# data.train_mask = data.val_mask =data.test_mask = None\n",
    "# print(data)\n",
    "\n",
    "# # print(tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e1ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = data.dataset.iloc[data.indices]\n",
    "# # # or\n",
    "# # df_train = df.iloc[data.indices]\n",
    "# df_train.head()\n",
    "import pandas as pd \n",
    "\n",
    "pd.DataFrame(train_data).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a18a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# px = pd.DataFrame(x.numpy())\n",
    "\n",
    "\n",
    "pd.DataFrame(data.y.numpy()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ef402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.edge_index[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855afad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"## Define the Encoder\")\n",
    "\n",
    "\n",
    "\n",
    "class GCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels, cached=True) # cached only for transductive learning\n",
    "        self.conv2 = GCNConv(2 * out_channels, out_channels, cached=True) # cached only for transductive learning\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "\n",
    "print(\"## Define the Autoencoder\")\n",
    "\n",
    "from torch_geometric.nn import GAE\n",
    "\n",
    "# parameters\n",
    "out_channels = 2\n",
    "num_features = dataset.num_features\n",
    "epochs = 100\n",
    "print({'num_features': num_features})\n",
    "\n",
    "# model\n",
    "model = GAE(GCNEncoder(num_features, out_channels))\n",
    "\n",
    "# move to GPU (if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "x = train_data.x.to(device)\n",
    "train_pos_edge_index = train_data.edge_index.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60082a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inizialize the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5258a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(x, train_pos_edge_index)\n",
    "    loss = model.recon_loss(z, train_pos_edge_index)\n",
    "    #if args.variational:\n",
    "    #   loss = loss + (1 / data.num_nodes) * model.kl_loss()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "def test(pos_edge_index, neg_edge_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(x, train_pos_edge_index)\n",
    "    return model.test(z, pos_edge_index, neg_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15ddb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train()\n",
    "\n",
    "    auc, ap = test(test_data.edge_index, test_data.edge_index)\n",
    "    print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))\n",
    "\n",
    "\n",
    "Z = model.encode(x, train_pos_edge_index)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a9800666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, AUC: 0.7833, AP: 0.7920\n",
      "Epoch: 002, AUC: 0.7761, AP: 0.7887\n",
      "Epoch: 003, AUC: 0.7629, AP: 0.7748\n",
      "Epoch: 004, AUC: 0.7636, AP: 0.7759\n",
      "Epoch: 005, AUC: 0.7713, AP: 0.7854\n",
      "Epoch: 006, AUC: 0.7782, AP: 0.7916\n",
      "Epoch: 007, AUC: 0.7816, AP: 0.7938\n",
      "Epoch: 008, AUC: 0.7822, AP: 0.7944\n",
      "Epoch: 009, AUC: 0.7816, AP: 0.7944\n",
      "Epoch: 010, AUC: 0.7792, AP: 0.7931\n",
      "Epoch: 011, AUC: 0.7758, AP: 0.7903\n",
      "Epoch: 012, AUC: 0.7730, AP: 0.7874\n",
      "Epoch: 013, AUC: 0.7731, AP: 0.7874\n",
      "Epoch: 014, AUC: 0.7754, AP: 0.7897\n",
      "Epoch: 015, AUC: 0.7791, AP: 0.7933\n",
      "Epoch: 016, AUC: 0.7818, AP: 0.7957\n",
      "Epoch: 017, AUC: 0.7835, AP: 0.7971\n",
      "Epoch: 018, AUC: 0.7841, AP: 0.7978\n",
      "Epoch: 019, AUC: 0.7838, AP: 0.7978\n",
      "Epoch: 020, AUC: 0.7825, AP: 0.7973\n",
      "Epoch: 021, AUC: 0.7810, AP: 0.7964\n",
      "Epoch: 022, AUC: 0.7791, AP: 0.7945\n",
      "Epoch: 023, AUC: 0.7789, AP: 0.7943\n",
      "Epoch: 024, AUC: 0.7810, AP: 0.7965\n",
      "Epoch: 025, AUC: 0.7835, AP: 0.7992\n",
      "Epoch: 026, AUC: 0.7851, AP: 0.8006\n",
      "Epoch: 027, AUC: 0.7865, AP: 0.8019\n",
      "Epoch: 028, AUC: 0.7875, AP: 0.8030\n",
      "Epoch: 029, AUC: 0.7881, AP: 0.8036\n",
      "Epoch: 030, AUC: 0.7874, AP: 0.8034\n",
      "Epoch: 031, AUC: 0.7862, AP: 0.8026\n",
      "Epoch: 032, AUC: 0.7860, AP: 0.8025\n",
      "Epoch: 033, AUC: 0.7867, AP: 0.8032\n",
      "Epoch: 034, AUC: 0.7880, AP: 0.8048\n",
      "Epoch: 035, AUC: 0.7901, AP: 0.8072\n",
      "Epoch: 036, AUC: 0.7928, AP: 0.8098\n",
      "Epoch: 037, AUC: 0.7945, AP: 0.8114\n",
      "Epoch: 038, AUC: 0.7957, AP: 0.8128\n",
      "Epoch: 039, AUC: 0.7966, AP: 0.8139\n",
      "Epoch: 040, AUC: 0.7972, AP: 0.8145\n",
      "Epoch: 041, AUC: 0.7980, AP: 0.8154\n",
      "Epoch: 042, AUC: 0.7986, AP: 0.8161\n",
      "Epoch: 043, AUC: 0.7998, AP: 0.8173\n",
      "Epoch: 044, AUC: 0.8011, AP: 0.8184\n",
      "Epoch: 045, AUC: 0.8031, AP: 0.8201\n",
      "Epoch: 046, AUC: 0.8052, AP: 0.8222\n",
      "Epoch: 047, AUC: 0.8073, AP: 0.8242\n",
      "Epoch: 048, AUC: 0.8093, AP: 0.8260\n",
      "Epoch: 049, AUC: 0.8111, AP: 0.8277\n",
      "Epoch: 050, AUC: 0.8124, AP: 0.8287\n",
      "Epoch: 051, AUC: 0.8138, AP: 0.8298\n",
      "Epoch: 052, AUC: 0.8154, AP: 0.8309\n",
      "Epoch: 053, AUC: 0.8170, AP: 0.8324\n",
      "Epoch: 054, AUC: 0.8190, AP: 0.8344\n",
      "Epoch: 055, AUC: 0.8207, AP: 0.8361\n",
      "Epoch: 056, AUC: 0.8226, AP: 0.8379\n",
      "Epoch: 057, AUC: 0.8243, AP: 0.8395\n",
      "Epoch: 058, AUC: 0.8262, AP: 0.8411\n",
      "Epoch: 059, AUC: 0.8273, AP: 0.8417\n",
      "Epoch: 060, AUC: 0.8288, AP: 0.8427\n",
      "Epoch: 061, AUC: 0.8300, AP: 0.8438\n",
      "Epoch: 062, AUC: 0.8315, AP: 0.8451\n",
      "Epoch: 063, AUC: 0.8324, AP: 0.8461\n",
      "Epoch: 064, AUC: 0.8333, AP: 0.8471\n",
      "Epoch: 065, AUC: 0.8344, AP: 0.8480\n",
      "Epoch: 066, AUC: 0.8354, AP: 0.8489\n",
      "Epoch: 067, AUC: 0.8363, AP: 0.8495\n",
      "Epoch: 068, AUC: 0.8374, AP: 0.8502\n",
      "Epoch: 069, AUC: 0.8382, AP: 0.8508\n",
      "Epoch: 070, AUC: 0.8387, AP: 0.8514\n",
      "Epoch: 071, AUC: 0.8394, AP: 0.8519\n",
      "Epoch: 072, AUC: 0.8400, AP: 0.8525\n",
      "Epoch: 073, AUC: 0.8402, AP: 0.8526\n",
      "Epoch: 074, AUC: 0.8402, AP: 0.8524\n",
      "Epoch: 075, AUC: 0.8407, AP: 0.8527\n",
      "Epoch: 076, AUC: 0.8411, AP: 0.8530\n",
      "Epoch: 077, AUC: 0.8416, AP: 0.8535\n",
      "Epoch: 078, AUC: 0.8418, AP: 0.8538\n",
      "Epoch: 079, AUC: 0.8419, AP: 0.8539\n",
      "Epoch: 080, AUC: 0.8417, AP: 0.8537\n",
      "Epoch: 081, AUC: 0.8418, AP: 0.8536\n",
      "Epoch: 082, AUC: 0.8417, AP: 0.8535\n",
      "Epoch: 083, AUC: 0.8413, AP: 0.8530\n",
      "Epoch: 084, AUC: 0.8409, AP: 0.8526\n",
      "Epoch: 085, AUC: 0.8408, AP: 0.8525\n",
      "Epoch: 086, AUC: 0.8408, AP: 0.8525\n",
      "Epoch: 087, AUC: 0.8409, AP: 0.8523\n",
      "Epoch: 088, AUC: 0.8408, AP: 0.8522\n",
      "Epoch: 089, AUC: 0.8411, AP: 0.8524\n",
      "Epoch: 090, AUC: 0.8412, AP: 0.8528\n",
      "Epoch: 091, AUC: 0.8407, AP: 0.8526\n",
      "Epoch: 092, AUC: 0.8404, AP: 0.8523\n",
      "Epoch: 093, AUC: 0.8402, AP: 0.8521\n",
      "Epoch: 094, AUC: 0.8402, AP: 0.8518\n",
      "Epoch: 095, AUC: 0.8403, AP: 0.8518\n",
      "Epoch: 096, AUC: 0.8400, AP: 0.8516\n",
      "Epoch: 097, AUC: 0.8402, AP: 0.8519\n",
      "Epoch: 098, AUC: 0.8400, AP: 0.8520\n",
      "Epoch: 099, AUC: 0.8399, AP: 0.8523\n",
      "Epoch: 100, AUC: 0.8396, AP: 0.8523\n",
      "Epoch: 101, AUC: 0.8394, AP: 0.8522\n",
      "Epoch: 102, AUC: 0.8393, AP: 0.8520\n",
      "Epoch: 103, AUC: 0.8392, AP: 0.8519\n",
      "Epoch: 104, AUC: 0.8392, AP: 0.8515\n",
      "Epoch: 105, AUC: 0.8393, AP: 0.8514\n",
      "Epoch: 106, AUC: 0.8395, AP: 0.8517\n",
      "Epoch: 107, AUC: 0.8397, AP: 0.8520\n",
      "Epoch: 108, AUC: 0.8396, AP: 0.8521\n",
      "Epoch: 109, AUC: 0.8389, AP: 0.8516\n",
      "Epoch: 110, AUC: 0.8385, AP: 0.8515\n",
      "Epoch: 111, AUC: 0.8379, AP: 0.8511\n",
      "Epoch: 112, AUC: 0.8379, AP: 0.8511\n",
      "Epoch: 113, AUC: 0.8385, AP: 0.8515\n",
      "Epoch: 114, AUC: 0.8388, AP: 0.8516\n",
      "Epoch: 115, AUC: 0.8393, AP: 0.8518\n",
      "Epoch: 116, AUC: 0.8393, AP: 0.8518\n",
      "Epoch: 117, AUC: 0.8388, AP: 0.8513\n",
      "Epoch: 118, AUC: 0.8380, AP: 0.8507\n",
      "Epoch: 119, AUC: 0.8379, AP: 0.8508\n",
      "Epoch: 120, AUC: 0.8381, AP: 0.8509\n",
      "Epoch: 121, AUC: 0.8386, AP: 0.8514\n",
      "Epoch: 122, AUC: 0.8389, AP: 0.8516\n",
      "Epoch: 123, AUC: 0.8391, AP: 0.8516\n",
      "Epoch: 124, AUC: 0.8391, AP: 0.8514\n",
      "Epoch: 125, AUC: 0.8390, AP: 0.8513\n",
      "Epoch: 126, AUC: 0.8385, AP: 0.8508\n",
      "Epoch: 127, AUC: 0.8381, AP: 0.8504\n",
      "Epoch: 128, AUC: 0.8382, AP: 0.8505\n",
      "Epoch: 129, AUC: 0.8379, AP: 0.8503\n",
      "Epoch: 130, AUC: 0.8376, AP: 0.8502\n",
      "Epoch: 131, AUC: 0.8375, AP: 0.8501\n",
      "Epoch: 132, AUC: 0.8373, AP: 0.8500\n",
      "Epoch: 133, AUC: 0.8375, AP: 0.8502\n",
      "Epoch: 134, AUC: 0.8380, AP: 0.8505\n",
      "Epoch: 135, AUC: 0.8384, AP: 0.8508\n",
      "Epoch: 136, AUC: 0.8383, AP: 0.8503\n",
      "Epoch: 137, AUC: 0.8381, AP: 0.8500\n",
      "Epoch: 138, AUC: 0.8377, AP: 0.8496\n",
      "Epoch: 139, AUC: 0.8377, AP: 0.8499\n",
      "Epoch: 140, AUC: 0.8381, AP: 0.8504\n",
      "Epoch: 141, AUC: 0.8380, AP: 0.8505\n",
      "Epoch: 142, AUC: 0.8380, AP: 0.8506\n",
      "Epoch: 143, AUC: 0.8376, AP: 0.8502\n",
      "Epoch: 144, AUC: 0.8376, AP: 0.8500\n",
      "Epoch: 145, AUC: 0.8374, AP: 0.8499\n",
      "Epoch: 146, AUC: 0.8373, AP: 0.8495\n",
      "Epoch: 147, AUC: 0.8373, AP: 0.8495\n",
      "Epoch: 148, AUC: 0.8377, AP: 0.8500\n",
      "Epoch: 149, AUC: 0.8378, AP: 0.8504\n",
      "Epoch: 150, AUC: 0.8376, AP: 0.8504\n",
      "Epoch: 151, AUC: 0.8374, AP: 0.8502\n",
      "Epoch: 152, AUC: 0.8370, AP: 0.8500\n",
      "Epoch: 153, AUC: 0.8366, AP: 0.8496\n",
      "Epoch: 154, AUC: 0.8371, AP: 0.8500\n",
      "Epoch: 155, AUC: 0.8368, AP: 0.8496\n",
      "Epoch: 156, AUC: 0.8365, AP: 0.8491\n",
      "Epoch: 157, AUC: 0.8362, AP: 0.8488\n",
      "Epoch: 158, AUC: 0.8360, AP: 0.8489\n",
      "Epoch: 159, AUC: 0.8358, AP: 0.8492\n",
      "Epoch: 160, AUC: 0.8354, AP: 0.8491\n",
      "Epoch: 161, AUC: 0.8354, AP: 0.8492\n",
      "Epoch: 162, AUC: 0.8354, AP: 0.8492\n",
      "Epoch: 163, AUC: 0.8349, AP: 0.8484\n",
      "Epoch: 164, AUC: 0.8344, AP: 0.8476\n",
      "Epoch: 165, AUC: 0.8337, AP: 0.8468\n",
      "Epoch: 166, AUC: 0.8332, AP: 0.8470\n",
      "Epoch: 167, AUC: 0.8327, AP: 0.8470\n",
      "Epoch: 168, AUC: 0.8324, AP: 0.8468\n",
      "Epoch: 169, AUC: 0.8331, AP: 0.8478\n",
      "Epoch: 170, AUC: 0.8338, AP: 0.8487\n",
      "Epoch: 171, AUC: 0.8339, AP: 0.8485\n",
      "Epoch: 172, AUC: 0.8335, AP: 0.8483\n",
      "Epoch: 173, AUC: 0.8331, AP: 0.8479\n",
      "Epoch: 174, AUC: 0.8325, AP: 0.8472\n",
      "Epoch: 175, AUC: 0.8324, AP: 0.8472\n",
      "Epoch: 176, AUC: 0.8323, AP: 0.8472\n",
      "Epoch: 177, AUC: 0.8326, AP: 0.8477\n",
      "Epoch: 178, AUC: 0.8330, AP: 0.8481\n",
      "Epoch: 179, AUC: 0.8328, AP: 0.8481\n",
      "Epoch: 180, AUC: 0.8327, AP: 0.8481\n",
      "Epoch: 181, AUC: 0.8322, AP: 0.8478\n",
      "Epoch: 182, AUC: 0.8321, AP: 0.8476\n",
      "Epoch: 183, AUC: 0.8322, AP: 0.8475\n",
      "Epoch: 184, AUC: 0.8322, AP: 0.8472\n",
      "Epoch: 185, AUC: 0.8319, AP: 0.8467\n",
      "Epoch: 186, AUC: 0.8318, AP: 0.8468\n",
      "Epoch: 187, AUC: 0.8315, AP: 0.8467\n",
      "Epoch: 188, AUC: 0.8316, AP: 0.8469\n",
      "Epoch: 189, AUC: 0.8317, AP: 0.8470\n",
      "Epoch: 190, AUC: 0.8320, AP: 0.8472\n",
      "Epoch: 191, AUC: 0.8316, AP: 0.8467\n",
      "Epoch: 192, AUC: 0.8307, AP: 0.8457\n",
      "Epoch: 193, AUC: 0.8297, AP: 0.8447\n",
      "Epoch: 194, AUC: 0.8294, AP: 0.8445\n",
      "Epoch: 195, AUC: 0.8300, AP: 0.8453\n",
      "Epoch: 196, AUC: 0.8300, AP: 0.8453\n",
      "Epoch: 197, AUC: 0.8297, AP: 0.8452\n",
      "Epoch: 198, AUC: 0.8288, AP: 0.8445\n",
      "Epoch: 199, AUC: 0.8279, AP: 0.8436\n",
      "Epoch: 200, AUC: 0.8271, AP: 0.8429\n"
     ]
    }
   ],
   "source": [
    "## rely on tensorboard \n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# parameters\n",
    "out_channels = 2\n",
    "num_features = dataset.num_features\n",
    "epochs = 200\n",
    "\n",
    "# model\n",
    "# model = GAE(GCNEncoder(num_features, out_channels))\n",
    "\n",
    "# move to GPU (if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "# x = data.x.to(device)\n",
    "# train_pos_edge_index = train_data.train_pos_edge_index.to(device)\n",
    "\n",
    "# inizialize the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "writer = SummaryWriter('runs/GAE1_experiment_'+'2d_200_epochs')\n",
    "\n",
    "\n",
    "# times = []\n",
    "# for epoch in range(1, 100 + 1):\n",
    "#     start = time.time()\n",
    "#     loss = train()\n",
    "#     auc, ap = test(test_data)\n",
    "#     print(f'Epoch: {epoch:03d}, AUC: {auc:.4f}, AP: {ap:.4f}')\n",
    "#     times.append(time.time() - start)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    auc, ap = test(test_data)\n",
    "    print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))\n",
    "\n",
    "\n",
    "    writer.add_scalar('auc train',auc,epoch) # new line\n",
    "    writer.add_scalar('ap train',ap,epoch)   # new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9c749d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torosx",
   "language": "python",
   "name": "torosx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
